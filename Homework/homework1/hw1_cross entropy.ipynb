{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"efH_hT3OHt2F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726415520078,"user_tz":-540,"elapsed":49913,"user":{"displayName":"이찬호","userId":"05908253026259697086"}},"outputId":"fc7cd670-2705-4cab-a8eb-226c4192a57a"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_data= [[9.46 8.48 7.74]\n"," [7.23 8.58 8.4 ]\n"," [5.73 3.82 9.95]\n"," [3.07 6.1  6.83]\n"," [6.49 4.28 9.98]\n"," [4.11 9.14 3.26]\n"," [8.69 7.11 8.81]\n"," [8.85 9.06 9.75]\n"," [9.73 7.29 9.59]\n"," [8.67 9.21 7.82]]\n","y_data= [[1. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.1250 - loss: 1.6629 - val_accuracy: 0.0000e+00 - val_loss: 1.9203\n","Epoch 2/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.1250 - loss: 1.3065 - val_accuracy: 0.0000e+00 - val_loss: 1.3948\n","Epoch 3/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 1.0252 - val_accuracy: 0.5000 - val_loss: 0.9660\n","Epoch 4/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8750 - loss: 0.8234 - val_accuracy: 1.0000 - val_loss: 0.6348\n","Epoch 5/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 0.7002 - val_accuracy: 1.0000 - val_loss: 0.4140\n","Epoch 6/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7500 - loss: 0.6434 - val_accuracy: 1.0000 - val_loss: 0.2837\n","Epoch 7/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.7500 - loss: 0.6274 - val_accuracy: 1.0000 - val_loss: 0.2155\n","Epoch 8/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6247 - val_accuracy: 1.0000 - val_loss: 0.1871\n","Epoch 9/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6181 - val_accuracy: 1.0000 - val_loss: 0.1836\n","Epoch 10/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6035 - val_accuracy: 1.0000 - val_loss: 0.1977\n","Epoch 11/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 0.5854 - val_accuracy: 1.0000 - val_loss: 0.2221\n","Epoch 12/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.5689 - val_accuracy: 1.0000 - val_loss: 0.2532\n","Epoch 13/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.5552 - val_accuracy: 1.0000 - val_loss: 0.2851\n","Epoch 14/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8750 - loss: 0.5429 - val_accuracy: 1.0000 - val_loss: 0.3034\n","Epoch 15/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8750 - loss: 0.5286 - val_accuracy: 1.0000 - val_loss: 0.3035\n","Epoch 16/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8750 - loss: 0.5104 - val_accuracy: 1.0000 - val_loss: 0.2885\n","Epoch 17/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8750 - loss: 0.4885 - val_accuracy: 1.0000 - val_loss: 0.2634\n","Epoch 18/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8750 - loss: 0.4654 - val_accuracy: 1.0000 - val_loss: 0.2370\n","Epoch 19/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8750 - loss: 0.4435 - val_accuracy: 1.0000 - val_loss: 0.2130\n","Epoch 20/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8750 - loss: 0.4239 - val_accuracy: 1.0000 - val_loss: 0.1947\n","Epoch 21/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8750 - loss: 0.4057 - val_accuracy: 1.0000 - val_loss: 0.1835\n","Epoch 22/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8750 - loss: 0.3886 - val_accuracy: 1.0000 - val_loss: 0.1795\n","Epoch 23/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8750 - loss: 0.3724 - val_accuracy: 1.0000 - val_loss: 0.1821\n","Epoch 24/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8750 - loss: 0.3573 - val_accuracy: 1.0000 - val_loss: 0.1909\n","Epoch 25/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8750 - loss: 0.3436 - val_accuracy: 1.0000 - val_loss: 0.2047\n","Epoch 26/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.3313 - val_accuracy: 1.0000 - val_loss: 0.2211\n","Epoch 27/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.3203 - val_accuracy: 1.0000 - val_loss: 0.2381\n","Epoch 28/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.3105 - val_accuracy: 1.0000 - val_loss: 0.2530\n","Epoch 29/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.3017 - val_accuracy: 1.0000 - val_loss: 0.2642\n","Epoch 30/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.2931 - val_accuracy: 1.0000 - val_loss: 0.2686\n","Epoch 31/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.2840 - val_accuracy: 1.0000 - val_loss: 0.2651\n","Epoch 32/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.2746 - val_accuracy: 1.0000 - val_loss: 0.2545\n","Epoch 33/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.2646 - val_accuracy: 1.0000 - val_loss: 0.2382\n","Epoch 34/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.2543 - val_accuracy: 1.0000 - val_loss: 0.2186\n","Epoch 35/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.2442 - val_accuracy: 1.0000 - val_loss: 0.1985\n","Epoch 36/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.2349 - val_accuracy: 1.0000 - val_loss: 0.1797\n","Epoch 37/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.2262 - val_accuracy: 1.0000 - val_loss: 0.1633\n","Epoch 38/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.2179 - val_accuracy: 1.0000 - val_loss: 0.1498\n","Epoch 39/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.2100 - val_accuracy: 1.0000 - val_loss: 0.1397\n","Epoch 40/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.2023 - val_accuracy: 1.0000 - val_loss: 0.1326\n","Epoch 41/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.1949 - val_accuracy: 1.0000 - val_loss: 0.1279\n","Epoch 42/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.1876 - val_accuracy: 1.0000 - val_loss: 0.1251\n","Epoch 43/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.1805 - val_accuracy: 1.0000 - val_loss: 0.1239\n","Epoch 44/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.1735 - val_accuracy: 1.0000 - val_loss: 0.1232\n","Epoch 45/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.1669 - val_accuracy: 1.0000 - val_loss: 0.1229\n","Epoch 46/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.1608 - val_accuracy: 1.0000 - val_loss: 0.1232\n","Epoch 47/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.1550 - val_accuracy: 1.0000 - val_loss: 0.1235\n","Epoch 48/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 0.1492 - val_accuracy: 1.0000 - val_loss: 0.1240\n","Epoch 49/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.1433 - val_accuracy: 1.0000 - val_loss: 0.1241\n","Epoch 50/50\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.1375 - val_accuracy: 1.0000 - val_loss: 0.1238\n","0 Accuracy :  0.2 Cost :  10.255037\n","500 Accuracy :  0.8 Cost :  0.66992265\n","1000 Accuracy :  0.8 Cost :  0.42405242\n","1500 Accuracy :  0.9 Cost :  0.33204612\n","2000 Accuracy :  1.0 Cost :  0.27964753\n","2500 Accuracy :  1.0 Cost :  0.24479239\n","3000 Accuracy :  1.0 Cost :  0.21959397\n","3500 Accuracy :  1.0 Cost :  0.20034726\n","4000 Accuracy :  1.0 Cost :  0.18505043\n","4500 Accuracy :  1.0 Cost :  0.17252028\n","5000 Accuracy :  1.0 Cost :  0.16201039\n"]}],"source":["# 4. Softmax classification\n","\n","# Import library\n","import numpy as np # 수치 연산을 위한 라이브러리\n","import tensorflow as tf # 머신러닝 프레임워크\n","import pandas as pd # 데이터 분석을 위한 라이브러리\n","from sklearn.preprocessing import OneHotEncoder # 카테고리 데이터를 one-hot encoding 하기 위한 라이브러리\n","from sklearn.model_selection import train_test_split\n","\n","# data load\n","df = pd.read_csv('triangular.csv') #triangular.csv' 파일을 읽어서 데이터프레임 df에 저장\n","\n","\n","# one-hot encoding\n","from sklearn.preprocessing import OneHotEncoder #OneHotEncoder: 객체를 생성하여 클래스 열을 one-hot encoding 함\n","encoder = OneHotEncoder(sparse_output=False) #sparse를 False로 설정하면 넘파이 배열을 반환하고 True로 설정하면 희소 행렬로 반환한다\n","encoded_data = encoder.fit_transform(df[['class']]) #fit_transform 메서드를 사용하여 클래스 데이터에 맞게 인코더를 학습하고, one-hot encoding 결과를 encoded_data에 저장\n","encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out()) #one-hot encoding 결과를 새로운 데이터프레임 encoded_df에 저장.\n","df = pd.concat([df, encoded_df], axis=1) #원본 데이터프레임 df에 one-hot encoding된 클래스 열을 추가하기 위해 concat 메서드를 사용\n","\n","\n","# 10개의 data만 선택\n","x_data = np.array(df.iloc[1:11,0:3], dtype=np.float32) #데이터프레임에서 1번째부터 10번째 행까지 처음 3열의 데이터를 선택하여 x_data에 NumPy 배열로 저장(특징 데이터).\n","y_data = np.array(df.iloc[1:11,4:], dtype=np.float32) #데이터프레임에서 1번째부터 10번째 행까지 4번째 열 이후의 모든 열 (one-hot encoding된 클래스 레이블)을 선택하여 y_data에 NumPy 배열로 저장(레이블 데이터).\n","print(\"x_data=\", x_data)\n","print(\"y_data=\", y_data)\n","\n","############## Multinomial regression model\n","# 데이터 분리\n","X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n","\n","\n","# num_classes 설정 (예시)\n","num_classes = y_data.shape[1]  # y_data의 열 개수를 클래스 개수로 설정\n","\n","\n","# 모델 생성\n","model = tf.keras.Sequential([ #tf.keras.Sequential: 층을 순차적으로 쌓아 모델을 생성하는 간단한 모델\n","    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)), # Dense() 첫 번째 은닉층을 정의\n","#128: 뉴런의 개수. 뉴런이 많을수록 더 복잡한 패턴을 학습할 수 있음\n","#activation='relu': 활성화 함수로 ReLU 함수를 사용. ReLU 함수는 음수 입력에 대해 0을 출력하고, 양수 입력에 대해서는 입력값을 그대로 출력.\n","#input_shape=(X_train.shape[1],): 입력 데이터의 형태를 지정. X_train.shape[1]은 입력 데이터의 특징 개수를 의미.\n","    tf.keras.layers.Dense(64, activation='relu'), #Dense() 두 번째 은닉층을 정의\n","    tf.keras.layers.Dense(num_classes, activation='softmax') #Dense(num_classes, activation='softmax'): 출력층을 정의\n","])\n","# num_classes는 클래스의 개수를 의미하며, softmax 활성화 함수를 사용하여 각 클래스에 대한 확률을 출력\n","\n","\n","\n","\n","# 모델 컴파일 model.compile: 모델을 학습하기 위한 설정\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), #옵티마이저로 Adam 옵티마이저를 사용하고, 학습률을 0.001로 설정 / Adam 옵티마이저는 일반적으로 빠르게 수렴하는 것으로 알려져 있음\n","              loss='categorical_crossentropy', #손실 함수로 categorical crossentropy를 사용 / 다중 클래스 분류 문제에서 일반적으로 사용되는 손실 함수\n","              metrics=['accuracy']) #모델 평가 시 정확도를 측정\n","\n","\n","# 모델 학습    model.fit: 모델을 학습시\n","\n","model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n","#print(\"model.fit=\", model.fit)\n","  #X_train, y_train: 학습 데이터와 레이블\n","  #epochs=50: 전체 데이터를 50번 반복하여 학습\n","  #batch_size=32: 한 번에 32개의 데이터를 사용하여 학습\n","  #validation_data=(X_test, y_test): 검증 데이터를 지정하여 학습 중에 모델의 성능을 평가\n","\n","\n","\n","# Model parameter\n","# 모델 파라미터를 작성하세요. / 가중치 행렬 W과 편향 벡터 b을 텐서플로우 변수 (Variable)로 정의\n","#tf.random.normal 함수를 사용하여 임의의 값으로 초기화\n","W = tf.Variable(tf.random.normal([3, 3])) #[3, 3]: 가중치 행렬의 크기는 입력 특징의 수 (3)와 클래스의 수 (3)를 반영\n","b = tf.Variable(tf.random.normal([3])) #[3]: 편향 벡터의 크기는 클래스의 수와 동일\n","#print(\"w=\", W)\n","#print(\"b=\", b)\n","# learning rate\n","learning_rate = 0.01 #학습률\n","\n","# Softmax classifier\n","def softmax_classifier(): # 소프트맥스 분류기를 정의하는 함수\n","    with tf.GradientTape() as tape: # 그래디언트 계산을 위한 컨텍스트를 생성\n","        # 변경된 코드\n","        logits = tf.matmul(x_data, W) + b # 입력 데이터 x_data와 가중치 행렬 W의 행렬곱에 편향 벡터 b를 더하여 로짓 값을 계산\n","        softmax_output = tf.nn.softmax(logits) # 로짓 값을 소프트맥스 함수에 적용하여 각 클래스에 대한 확률을 계산\n","\n","        # Cross-entropy (라이브러리 사용 X)\n","        cost = -tf.reduce_mean(tf.reduce_sum(y_data * tf.math.log(softmax_output), axis=1)) # 교차 엔트로피 손실을 계산\n","        # (y_data * tf.math.log(softmax_output): 각 클래스에 대한 실제 레이블과 예측 확률의 로그의 곱을 계산\n","        # tf.reduce_sum(..., axis=1): 각 샘플에 대해 클래스별 손실을 합산\n","        # tf.reduce_mean(...): 전체 샘플에 대한 평균 손실을 계산\n","\n","\n","\n","        # Gradient 계산\n","        gradients = tape.gradient(cost, [W, b]) #손실 함수에 대한 가중치와 편향의 그래디언트를 계산\n","        #print(\"gradients=\", gradients)\n","        # Weight update\n","        tf.optimizers.SGD(learning_rate).apply_gradients(zip(gradients, [W, b])) #경사하강법 옵티마이저를 사용하여 가중치와 편향을 업데이트\n","\n","        # 손실 값 반환\n","        return cost\n","\n","############### Learning\n","for step in range(5001): #5001번의 학습 반복을 수행\n","    cost = softmax_classifier() #소프트맥스 분류기 함수를 호출하여 손실 값을 계산하고 반환\n","\n","    if step % 500 == 0: #500번마다 학습 진행 상황을 출력\n","        # Softmax 분류기를 작성하세요.\n","        model_LC = tf.matmul(x_data,W) + b #모델의 출력을 계산\n","        model = tf.argmax(tf.nn.softmax(model_LC),1) #출력값을 클래스 인덱스로 변환\n","\n","        # accuracy 계산 /  accuracy, cost 출력\n","        accuracy = tf.reduce_mean(tf.cast(tf.equal(model, tf.argmax(y_data,1)),tf.float32)) #정확도를 계산\n","        print(step, 'Accuracy : ', accuracy.numpy(), 'Cost : ', cost.numpy()) #학습 단계, 정확도, 손실 값을 출력\n"]},{"cell_type":"code","source":[],"metadata":{"id":"tC6aEZkb2xnB"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1q3_AT1Ktrf8puw-aipvhz3dElgdZYm-O","timestamp":1725937874125}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}